{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4265 - Computer Vision & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 Report - Group 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dionysios Rigatos <br />\n",
    "> dionysir@stud.ntnu.no <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Assignment01 was delivered by me while part of Group 113. I am now part of Group 66."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate $\\alpha$ will be thus defined as $\\eta$ for this task, as a personal preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show that $w_{ji} := w_{ji} - \\eta \\frac{\\partial C}{\\partial w_{ji}}$ is equivalent to $w_{ji} := w_{ji} - \\eta \\delta_j x_i$. \n",
    "\n",
    "Given $\\delta_j = \\frac{\\partial C}{\\partial z_j}$, we will show that $\\frac{\\partial C}{\\partial w_{ji}} = \\delta_j x_i$.\n",
    "\n",
    "The assumption that the bias term is included in the weights is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, we can write:\n",
    "\n",
    "* $\\delta_j = \\frac{\\partial C}{\\partial z_j} = \\frac{\\partial C}{\\partial a_j} \\frac{\\partial a_j}{\\partial z_j} (1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can expand:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial w_{ji}} = \\frac{\\partial C}{\\partial a_j} \\frac{\\partial a_j}{\\partial z_j} \\frac{\\partial z_j}{\\partial w_{ji}} (2)$\n",
    "\n",
    "And given $(1)$, we can write:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial w_{ji}} = \\delta_j \\frac{\\partial z_j}{\\partial w_{ji}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we can expand...\n",
    "\n",
    "* $\\frac{\\partial z_j}{\\partial w_{ji}} = \\frac{\\partial (w_{ji}x_i)}{\\partial w_{ji}} = x_i (3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus, from $(1)$ and $(3)$ we have that $(2)$ becomes:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial w_{ji}} = \\delta_j x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write:\n",
    "\n",
    "* $w_{ji} := w_{ji} - \\eta \\frac{\\partial C}{\\partial w_{ji}} = w_{ji} - \\eta \\delta_j x_i$ \n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the second part;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show that $\\delta_j = f'(z_j) \\sum_k w_{kj} \\delta_k$, given $\\delta_k = \\frac{\\partial C}{\\partial z_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by re-writing our equation as $\\delta_j = \\frac{\\partial C}{\\partial z_j} = \\frac{\\partial C}{\\partial a_j} \\frac{\\partial a_j}{\\partial z_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to show:\n",
    "\n",
    "* $\\frac{\\partial a_j}{\\partial z_j} = f'(z_j)$ $(1)$\n",
    "* $\\frac{\\partial C}{\\partial a_j} = \\sum_k w_{kj} \\delta_k$ $(2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $(1)$, we can write:\n",
    "\n",
    "* $\\frac{\\partial a_j}{\\partial z_j} = \\frac{\\partial f(z_j)}{\\partial z_j} = f'(z_j)$ $(3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for (2), we can write, using the chain rule and knowing that the hidden layer $j$ is connected to output layer $k$:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial a_j} = \\frac{\\partial C}{\\partial a_k} \\frac{\\partial a_k}{\\partial z_k} \\frac{\\partial z_k}{\\partial a_j}$ $(4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that:\n",
    "\n",
    "* $ \\frac{\\partial C}{\\partial a_k} \\frac{\\partial a_k}{\\partial z_k} = \\delta_k$ from the definition of $\\delta_k$ $(5)$\n",
    "\n",
    "* $\\frac{\\partial z_k}{\\partial a_j} = \\frac{\\partial (w_{kj}a_j)}{\\partial a_j} = w_{kj}$ $(6)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now $(4)$ becomes:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial a_j} = w_{kj} \\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generalize for all the output layers, we can re-write $(4)$ as:\n",
    "\n",
    "* $\\frac{\\partial C}{\\partial a_j} = \\sum_k w_{kj} \\delta_k$ $(7)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using $(3)$ and $(7)$, we can write the desired equation:\n",
    "\n",
    "* $\\delta_j = f'(z_j) \\sum_k w_{kj} \\delta_k$\n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to re-write $w_{ji} := w_{ji} - \\eta \\frac{\\partial C}{\\partial w_{ji}}$ and $w_{kj} := w_{kj} - \\eta \\frac{\\partial C}{\\partial w_{kj}}$ in matrix notation, for the whole layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the input layer has $I$ neurons, the hidden layer has $J$ neurons and the output layer has $K$ neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with $W^{J}$ for the hidden layer.\n",
    "\n",
    "* $W^{J} := W^{J} - \\eta \\frac{\\partial C}{\\partial W^{J}}$\n",
    "\n",
    "Where $W^{J}$ is the matrix of weights connecting the input layer to the hidden layer, and $\\frac{\\partial C}{\\partial W^{J}}$ is the matrix of the partial derivatives of the cost function with respect to the weights connecting the input layer to the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizes are as follows:\n",
    "\n",
    "* $W^{J}$ is of size $J \\times I$\n",
    "* $\\frac{\\partial C}{\\partial W^{J}}$ is of size $J \\times I$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the output layer $W^{K}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $W^{K} := W^{K} - \\eta \\frac{\\partial C}{\\partial W^{K}}$\n",
    "\n",
    "Where $W^{K}$ is the matrix of weights connecting the hidden layer to the output layer, and $\\frac{\\partial C}{\\partial W^{K}}$ is the matrix of the partial derivatives of the cost function with respect to the weights connecting the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizes are as follows:\n",
    "\n",
    "* $W^{K}$ is of size $K \\times J$\n",
    "* $\\frac{\\partial C}{\\partial W^{K}}$ is of size $K \\times J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is μ: 33.55274553571429 and the standard deviation is σ: 78.87550070784701."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2c_img](images/task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network has 785 input nodes, 64 hidden nodes and 10 output nodes. \n",
    "\n",
    "Our parameters in this case are $785*64 + 64*10 = 50880$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a) Weight Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3a_img](images/task3_True_False_False.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the modified model the validation loss is significantly lower compare to the base model - something that is reflected in the accuracy as well. \n",
    "\n",
    "Initializing the weights from a normal distribution with a standard deviation of 1 means our weights are not too small or too large, and thus the learning process is more efficient and the model is able to learn better as it focuses on the linear part of the sigmoid function. Additionally, having them zero-centered decreases any potential bias in the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b) Improved Sigmoid w/ Weight Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3b_img](images/task3_True_True_False.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the improved sigmoid function in combination with the weights initialization trick, we can see that the validation loss is significantly lower compare to the base model - something that is reflected in the accuracy as well. \n",
    "\n",
    "The improved sigmoid produces outputs that are closer to zero, thus providing the next layers with normalized inputs. The linear term 1.7159 helps us avoid flat regions in the sigmoid function, and thus the learning process is more efficient as we notice a faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c) Momentum w/ Improved Sigmoid and Weight Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3c_img](images/task3_True_True_True.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the momentum in combination with the improved sigmoid function and the weights initialization trick, we can see that the validation loss is significantly lower compared to the base model and an even faster convergence than previously achieved.\n",
    "\n",
    "The loss flattens out much quicker than before, and the accuracy is higher than before. This is due to the fact that the momentum allows the gradient to descent faster in flat regions and slower in steep regions, thus avoiding local minima and converging faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum, these Tricks of the Trade significantly improve both the accuracy and the convergence speed of the model - and their benefits are cumulative. However, generalization is not really improved as we notice a large gap between the training and validation loss and accuracy - thus overfitting is still an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Default Model Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4_default_img](images/task4_default_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the default model with **1 hidden layer and 64 hidden units** has a validation loss of around 0.15 and a training loss of around 0.002. The accuracy is 0.96 and 1.0 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvements implemented earlier are enabled in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4a_img](images/task4a_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 32 hidden units, we can see that the validation loss is around 0.22 and the training loss is around 0.022. The accuracy is 0.94 and 0.99 respectively.\n",
    "\n",
    "We see a slight decrease in the accuracy and an increase in the loss, which is expected as the model has less capacity to learn due to the reduced number of hidden units. Specifically, the model is not able to capture the complexity of the data as well as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4b_img](images/task4b_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 128 hidden units, we can see that the validation loss is around 0.12 and the training loss is around 0.0008. The accuracy is 0.97 and 1.0 respectively.\n",
    "\n",
    "While the accuracy and the training loss are both improved, the validation loss is not *significantly* improved. This is due to the fact that the model is overfitting the training data by capturing too much of its complexity, and thus it is not able to generalize well to the validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task 3, our model had $50880$ parameters. In order to replicate this amount of total parameters with 2 equally sized hidden layers instead of 1, we'd need to make them of size $59$ each. This is because the input layer has $785$ neurons, and the output layer has $10$ neurons. Thus, the total number of parameters is $785*59 + 59*59 + 59*10 = 50386$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4d_img](images/task4d_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation loss is around 0.16 and the training loss is around 0.0008. The accuracy is 0.96 and 1.0 respectively.\n",
    "\n",
    "The model performs slightly worse (negligible) than the 128 hidden unis - 1 hidden layer one. This is because training loss and accuracy have peaked and the model was already overfitting the training data. Validation, however, has very slightly worsened, therefore the model is not able to generalize as well as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained for 25 epochs, although early stopping kicked in at 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4e_img](images/task4e_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the validation loss is around 0.23 and the training loss is around 0.07. The accuracy is 0.97 and 0.94 respectively.\n",
    "\n",
    "This model, despite its impressive size, is overfitting so much that the performance has actually deteriorated instead of plateauing. Due to the amount of hidden layers, the weights take longer to converge to a minimum, and not only is training slower but the model does not generalize well at all as it deeply tries to replicate the training data at every training step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
