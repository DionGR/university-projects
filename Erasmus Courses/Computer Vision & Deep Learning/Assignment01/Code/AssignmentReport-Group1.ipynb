{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4265 - Computer Vision & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 Report - Group 113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dionysios Rigatos <br />\n",
    "> dionysir@stud.ntnu.no <br />\n",
    "> Joel Constantinos <br />\n",
    "> joelc@stud.ntnu.no <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this proof, we want to show that:\n",
    "\n",
    "$\\frac{\\partial C^n(w)}{\\partial w_i} =-(y^n - \\hat{y}^n)x_i^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{y}^n = f(x^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, we can write the derivative of the cost function as:\n",
    "* $\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} \\frac{\\partial \\hat{y}^n}{\\partial w_i}$\n",
    "\n",
    "We already know that:\n",
    "* $C^n(w) = -y^n \\ln(\\hat{y}^n) - (1-y^n) \\ln(1-\\hat{y}^n)$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial \\hat{y}^n}{\\partial w_i} = x_i^n \\hat{y}^n(1 −\\hat{y}^n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is left is to find $\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n}$:\n",
    "\n",
    "$\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} =$\n",
    "\n",
    "$=\\frac{\\partial (-y^n \\ln(\\hat{y}^n) - (1-y^n) \\ln(1-\\hat{y}^n))}{\\partial \\hat{y}^n}=$\n",
    "\n",
    "$=-\\frac{y^n}{\\hat{y}^n}-(-\\frac{(1-y^n)}{(1-\\hat{y}^n)})=$\n",
    "\n",
    "$=-\\frac{y^n}{\\hat{y}^n}+\\frac{(1-y^n)}{(1-\\hat{y}^n)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have:\n",
    "* $\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} = -\\frac{y^n}{\\hat{y}^n}+\\frac{(1-y^n)}{(1-\\hat{y}^n)}$ \n",
    "\n",
    "* $\\frac{\\partial \\hat{y}^n}{\\partial w_i} = x_i^n \\hat{y}^n(1 −\\hat{y}^n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the derivative of the cost function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C^n(w)}{\\partial w_i} =$\n",
    "\n",
    "$=\\frac{\\partial C^n(w)}{\\partial \\hat{y}^n} \\frac{\\partial \\hat{y}^n}{\\partial w_i} =$\n",
    "\n",
    "$=(-\\frac{y^n}{\\hat{y}^n}+\\frac{(1-y^n)}{(1-\\hat{y}^n)})(x_i^n \\hat{y}^n(1 −\\hat{y}^n))=$\n",
    "\n",
    "$= -\\frac{x_i^ny^n\\hat{y}^n(1-\\hat{y}^n)}{\\hat{y}^n} + -\\frac{x_i^n\\hat{y}^n(1-y^n)(1-\\hat{y}^n)}{1-\\hat{y}^n} =$\n",
    "\n",
    "$=-x_i^ny^n(1-\\hat{y}^n) + x_i^n\\hat{y}^n(1-y^n) =$\n",
    "\n",
    "$=-x_i^n[y^n(1-\\hat{y}^n) - \\hat{y}^n(1-y^n)] =$\n",
    "\n",
    "$=-x_i^n(y^n - \\hat{y}^n - \\hat{y}^ny^n + \\hat{y}^ny^n) = $\n",
    "\n",
    "$=-x_i^n(y^n - \\hat{y}^n) = $\n",
    "\n",
    "$=-(y^n - \\hat{y}^n)x_i^n$ \n",
    "\n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task2/task2b_training_and_validation_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task2/task2c_training_and_validation_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The early stopping kicked in at epoch number 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task2/task2e_validation_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that dataset shuffling has significantly improved convergence speed of the validation set opposed to the unshuffled runs (with early stopping on in both cases). The spikes we observed during unshuffled validation are indication of a problematic batch that consistently dunks the validation accuracy - a pattern that is not present in the shuffled runs as the data is not presented in the same order every time. Consequently, the model is trained on a more diverse set of data and the gradient updates are more consistent and eliminate the spikes early on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task3/task3b_training_and_validation_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task3/task3c_training_and_validation_accurcy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, we can see that there are signs of divergence between the training and validation accuracy - with the training accuracy increasing and the validation accuracy reaching a plateau. While **this may be an early sign of overfitting**, looking at the scale of the initial plot, the difference is not significant. This becomes more evident when we look at the zoomed out accuracy plot, where the trend seems reasonable - a slightly higher training accuracy than validation accuracy with a very small gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task3/task3c_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this proof, we want to derive the update term for L2 regularization: \n",
    "\n",
    "- $\\frac{\\partial J(w)}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $J(w) = C(w) + \\lambda R(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have:\n",
    "\n",
    "- $\\frac{\\partial J(w)}{\\partial w} = \\frac{\\partial C(w)}{\\partial w} + \\frac{\\partial \\lambda R(w)}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know from Task (1a) that:\n",
    "- $\\frac{\\partial C^n(w)}{\\partial w_{kj}} =-(y_k^n - \\hat{y_k}^n)x_j^n$\n",
    "\n",
    "So the derivative of $C(w_{kj})$ with respect to $w_{kj}$, for the entire dataset, is:\n",
    "- $\\frac{\\partial C(w)}{\\partial w_{kj}} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial C^n(w)}{\\partial w_{kj}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to find the derivative of the L2 regularization term:\n",
    "\n",
    "$\\frac{\\partial R(w)}{\\partial w_{kj}} =$\n",
    "\n",
    "$\\frac{\\partial \\sum_{i=1, j=1}^{N} w_{ij}^2}{\\partial w_{kj}} =$\n",
    "\n",
    "$= 2w_{kj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, finally, we have:\n",
    "\n",
    "$\\frac{\\partial J(w)}{\\partial w_{kj}} = \\frac{\\partial C(w)}{\\partial w_{kj}} + \\frac{\\partial \\lambda R(w)}{\\partial w_{kj}} =$ \n",
    "\n",
    "$= \\frac{1}{N} \\sum_{n=1}^{N} -(y_k^n - \\hat{y_k}^n)x_j^n + 2\\lambda w_{kj}$\n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task4/task4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the model with the regularized weights has significantly less noise. \n",
    "\n",
    "Due to the L2 regularization penalty, the weights are kept small. This leads to a reduction in the variance of the model, which in turn reduces the noise in the training and validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task4/task4c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Early stopping was not used in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is supposed to improve the generalization of the model, and the validation accuracy is a good indicator of this.\n",
    "\n",
    "However, this is not the case here. This might be because the dataset is too small, and the model is not complex enough so as to benefit from regularization.\n",
    "\n",
    "Another reason for decreasing accuracy as lambda increases is that the regularization strength increases. With a higher lambda, the penalty induced on the weight matrix is much stronger and the model ends up being more focused on minimizing the weights rather than the loss function. \n",
    "\n",
    "This results in a model that is less capable of generalizing to the validation set, and the validation accuracy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training and validation loss](../images/task4/task4e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we increase the lambda regularization value, the more the model is penalized for having large weights.\n",
    "\n",
    "This is evident in the L2 norm of the weights, which decreases significantly as the lambda value increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
