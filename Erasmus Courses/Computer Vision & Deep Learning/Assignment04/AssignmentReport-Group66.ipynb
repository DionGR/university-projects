{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4265 - Computer Vision & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 Report - Group 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dionysios Rigatos <br />\n",
    "> dionysir@stud.ntnu.no <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection over Union is a metric that measures the overlap between two bounding boxes (usually between our prediction against a ground truth) in object detection models. \n",
    "\n",
    "The formula for calculating IoU for two bounding boxes $X$ and $Y$ is:\n",
    "\n",
    "$IoU = \\frac{area(X \\cap Y)}{area(X \\cup Y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iou](./task2/iou.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the area of intersection and the area of union. Of course, this would be a bad example of IoU, since the intersection is very small compared to the union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "\n",
    "* $Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "* $Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Where $TP$ is the amount of $True Positives$, $FP$ is the amount of $False Positives$ and $FN$ is the amount of $False Negatives$.\n",
    "\n",
    "A $True Positive$ is a prediction that was correctly assigned a label/bounding box - in our case, a bounding box with IoU >= threshold.\n",
    "\n",
    "A $False Positive$ is a prediction that was incorrectly assigned a label/bounding box - in our case, a bounding box with IoU < threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following precision and recall curve for the two classes, what is the mean average\n",
    "precision?\n",
    "Precision and recall curve for class 1:\n",
    "Precision1 = [1.0, 1.0, 1.0, 0.5, 0.20]\n",
    "Recall1 = [0.05, 0.1, 0.4, 0.7, 1.0]\n",
    "Precision and recall curve for class 2:\n",
    "Precision2 = [1.0, 0.80, 0.60, 0.5, 0.20]\n",
    "Recall2 = [0.3, 0.4, 0.5, 0.7, 1.0]\n",
    "Hint: To calculate this, find the precision for the following recall levels: 0.0, 0.1, 0.2, ... 0.9, 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the mAP (mean Average Precision) we'll calculate the precision for the recall interval [0, 1] with step 0.1.\n",
    "\n",
    "* For Class_1, we have the following precisions per interval:\n",
    "    * Recall 0.0 - Precision 1.0\n",
    "    * Recall 0.1 - Precision 1.0\n",
    "    * Recall 0.2 - Precision 1.0\n",
    "    * Recall 0.3 - Precision 1.0\n",
    "    * Recall 0.4 - Precision 1.0\n",
    "    * Recall 0.5 - Precision 0.5\n",
    "    * Recall 0.6 - Precision 0.5\n",
    "    * Recall 0.7 - Precision 0.5\n",
    "    * Recall 0.8 - Precision 0.2\n",
    "    * Recall 0.9 - Precision 0.2\n",
    "    * Recall 1.0 - Precision 0.2\n",
    "    \n",
    "* So the AP for Class_1 is around 0.65.\n",
    "\n",
    "* For Class_2, we have the following precisions per interval:\n",
    "    * Recall 0.0 - Precision 1.0\n",
    "    * Recall 0.1 - Precision 1.0\n",
    "    * Recall 0.2 - Precision 1.0\n",
    "    * Recall 0.3 - Precision 1.0\n",
    "    * Recall 0.4 - Precision 0.8\n",
    "    * Recall 0.5 - Precision 0.6\n",
    "    * Recall 0.6 - Precision 0.6\n",
    "    * Recall 0.7 - Precision 0.5\n",
    "    * Recall 0.8 - Precision 0.2\n",
    "    * Recall 0.9 - Precision 0.2\n",
    "    * Recall 1.0 - Precision 0.2 \n",
    "\n",
    "* So the AP for Class_2 is around 0.71.\n",
    "\n",
    "\n",
    "So the mAP is the average of the APs for each class, which is around 0.68. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Task 2f](./task2/precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Picking the best box for our ground-truth label requires a matching strategy that will maximize the IoU between the predicted bounding box and the ground-truth bounding box. The best box is the one that maximizes the IoU.\n",
    "\n",
    "### Task 3b)\n",
    "False. The input image's resolution is higher in earlier layers, thus the bounding boxes capture a smaller area of the picture - thus detecting objects of smaller sizes. As the layers progress, the resolution decreases and one bounding box is able to capture more information - thus detecting larger objects.\n",
    "\n",
    "### Task 3c)\n",
    "By using different bounding box aspect ratios we allow the model to capture a larger variety of objects. Using a single aspect ratio, such as a square, would inhibit the model's ability to detect objects that are wider or taller - such as cars or people.\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "SSD eliminates region proposal networks by using a fixed set of bounding boxes at different scales and aspect ratios unlike YOLO which uses a single bounding box for each grid cell. YOLO also works on a single scale, while SSD uses multiple scales to detect objects of different sizes.\n",
    "\n",
    "### Task 3e)\n",
    "If the feature map is of size 38x38 and the number of default boxes is 6, then the total number boxes is 38x38x6 = 8664 for this feature map.\n",
    "\n",
    "### Task 3f)\n",
    "For each feature map we have:\n",
    "* 38x38x6 = 8664 boxes\n",
    "* 19x19x6 = 2166 boxes\n",
    "* 10x10x6 = 600 boxes\n",
    "* 5x5x6 = 150 boxes\n",
    "* 3x3x6 = 54 boxes\n",
    "* 1x1x6 = 6 boxes\n",
    "\n",
    "So the total number of boxes is 8664 + 2166 + 600 + 150 + 54 + 6 = 11640 boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "\n",
    "![Task 4b](./task2/loss_4b.png)\n",
    "\n",
    "We achieved a $mAP@0.5$ of $0.791$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4c)\n",
    "\n",
    "![Task 4c](./task2/task4c.png)\n",
    "\n",
    "The final achieved $mAP@0.5$ was approximately $0.9128$.\n",
    "\n",
    "Improvements done were:\n",
    "* Used batch normalization.\n",
    "* Added another, larger feature map (76x76) for detecting smaller objects.\n",
    "* Used PReLU activation function instead of ReLU.\n",
    "* Adam optimizer with a learning rate half of the original one.\n",
    "\n",
    "No augmentation was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4d)\n",
    "\n",
    "There was no time to implement the extra task. However, I have already completed all the mandatory assignments with the required grade (75% on 3/4) so it should not be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4e)\n",
    "\n",
    "There was no time to implement the extra task. However, I have already completed all the mandatory assignments with the required grade (75% on 3/4) so it should not be an issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
