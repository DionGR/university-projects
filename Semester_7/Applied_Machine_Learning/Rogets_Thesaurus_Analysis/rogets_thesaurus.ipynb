{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Information\n",
    "---\n",
    "\n",
    "> Dionysios Rigatos <br />\n",
    "> Department of Informatics  <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> p3200262@aueb.gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roget's Thesaurus in the 21st Century\n",
    "\n",
    "The first known thesaurus was written in the 1st century CE by [Philo of Byblos](https://en.wikipedia.org/wiki/Philo_of_Byblos); it was called *Περὶ τῶν διαφόρως σημαινομένων*, loosly translated in English as *On Synonyms*. Fast forward about two millenia and we arrive to the most well known thesaurus, compiled by [Peter Mark Roget](https://en.wikipedia.org/wiki/Peter_Mark_Roget), a British physician, natural theologian, and lexicographer. [Roget's Thesaurus](https://en.wikipedia.org/wiki/Roget%27s_Thesaurus) was released on 29 April 1852, containing 15,000 words. Subsequent editions were larger, with the latest totalling 443,000 words. In Greek the most well known thesaurus, *Αντιλεξικόν ή Ονομαστικόν της Νεοελληνικής Γλώσσης* was released in 1949 by [Θεολόγος Βοσταντζόγλου](https://el.wikipedia.org/wiki/%CE%98%CE%B5%CE%BF%CE%BB%CF%8C%CE%B3%CE%BF%CF%82_%CE%92%CE%BF%CF%83%CF%84%CE%B1%CE%BD%CF%84%CE%B6%CF%8C%CE%B3%CE%BB%CE%BF%CF%85); the latest updated edition was released in 2008 and remains an indispensable source for writing in Greek.\n",
    "\n",
    "Roget organised the entries of the thesaurus in a hierarchy of categories. Your task in this assignment is to investigate how these categories fare with the meaning of English words as captured by Machine Learning techniques, namely, their embeddings.\n",
    "\n",
    "Note that this is an assignment that requires initiative and creativity from your part. There is no simple right or wrong answer. It is up to you to find the best solution. You have three weeks to do it. Make them count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Visualization Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ML Model & Data Preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score,\n",
    "                             homogeneity_score, completeness_score, v_measure_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import umap\n",
    "\n",
    "# Imbalanced Dataset Handling\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Input, PReLU)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, LearningRateScheduler)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalFocalCrossentropy \n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy\n",
    "\n",
    "# External Model APIs\n",
    "import voyageai\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If there is an issue with loading the embeddings, feel free to use my API keys.\n",
    "\n",
    "* If the embedding file is not found, the embeddings will be fetched automatically using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyage_client = voyageai.Client(api_key=\"pa-JukUHSiLV-1AH4xpPymGslB9Hyxi-XB58rS0eMXllWw\")\n",
    "mistral_client = MistralClient(api_key=\"XWJS5gJBfw6tQflBDpUh0SjP1rToTixZ\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roget's Thesaurus Classification\n",
    "\n",
    "You can find [Roget's Thesaurus classification online at the Wikipedia](https://en.wiktionary.org/wiki/Appendix:Roget%27s_thesaurus_classification). You must download the categorisation (and the words belonging in each category), save them and store them in the way that you deem most convenient for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fetch the Thesaurus\n",
    "\n",
    "* Initially we want to fetch the thesaurus from the [Project Gutenber Roget's Thesaurus](https://www.gutenberg.org/cache/epub/22/pg22-images.html). \n",
    "\n",
    "* We will do this using BeautifulSoup as it is a simple and easy to use library for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://www.gutenberg.org/cache/epub/22/pg22-images.html\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(dataset_url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(f\"Page retrieval OK with Status Code ({response.status_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Classes, Sections and Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once we have the scraped HTML, we want to extract the classes, sections and entries from the thesaurus.\n",
    "\n",
    "* Initially we will only extract the necessary information without performing any text processing techniques on the data, therefore it will be in a raw format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a quick inspection of the HTML, we can see that:\n",
    "\n",
    "* There are multiple `h2` tags which might not contain relevant information.\n",
    "* Divisions, despite being hierarchically under Classes, are not directly under them. They are under `h2` tags along with them.\n",
    "* Sections are under divisions as `h3` tags.\n",
    "* Entries are under sections as `p` tags, belonging to the `p2` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<h2>ROGET’S THESAURUS<br>OF<br>ENGLISH WORDS AND PHRASES</h2>\n",
    "\n",
    "<h2><a id=\"class01\"></a>CLASS I<br>\n",
    "WORDS EXPRESSING ABSTRACT RELATIONS</h2>\n",
    "<h3><a id=\"sect01\"></a>S<small>ECTION</small> I. EXISTENCE</h3>\n",
    "<h4>1. BEING, IN THE ABSTRACT</h4>\n",
    "<p class=\"p2\">\n",
    "<b>#1. Existence.—N.</b> existence, being, entity, <i>ens</i>, <i>esse</i>,\n",
    "subsistence.<br>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A convention\n",
    "* Since we wish to classify using two levels, if a Class contains Divisions, then the Divisions will act as the second hierarchy and contain all the words their Secitons had. If a Class does not contain Divisions, then the Sections will act as the second hierarchy and contain all the words they originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "\n",
    "# Find all classes and divisions\n",
    "h2_headers = soup.find_all(\"h2\", id=None)\n",
    "relevant_h2 = [h2 for h2 in h2_headers if re.search(\"class|division\", h2.text, re.IGNORECASE)]\n",
    "\n",
    "current_class = \"\"\n",
    "\n",
    "for header in relevant_h2:\n",
    "    header_text = header.text.strip()\n",
    "    \n",
    "    # Determine if it is a class or division\n",
    "    if \"class\" in header_text.lower():\n",
    "        current_class = header_text\n",
    "        class_data = {}\n",
    "        \n",
    "        # Find all sections within each class only\n",
    "        class_sections = [section for section in header.find_all_next('h3') if section.find_previous('h2') == header]\n",
    "        \n",
    "        # Iterate over each section\n",
    "        for section in class_sections:\n",
    "            section_title = section.text\n",
    "                    \n",
    "            # Find all 'p\" belonging to this section only\n",
    "            entries = [entry.text for entry in section.find_all_next('p', class_=\"p2\") if entry.find_previous('h3') == section]\n",
    "            \n",
    "            class_data[section_title] = entries\n",
    "            \n",
    "        raw_data[current_class] = class_data\n",
    "        \n",
    "    # Divisions will act as sections within the class\n",
    "    elif \"division\" in header_text.lower():\n",
    "        if current_class:  \n",
    "            section_title = header_text\n",
    "            \n",
    "            entries = [entry.text for entry in header.find_all_next('p', class_=\"p2\") if entry.find_previous('h2') == header]\n",
    "            \n",
    "\n",
    "        raw_data[current_class][section_title] = entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will store our extracted, yet **unprocessed** data in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('thesaurus_raw.json', 'w') as json_file:\n",
    "    json.dump(raw_data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a look out our hierarchy as well as how many entries are contained per Section.\n",
    "\n",
    "* We notice that a Class might have multiple Sections with the same numbering; this is because we flattened the Divisions.\n",
    "\n",
    "* Each entry is a word along with all of its synonym phrases - it is still a single string as it is unprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_structure(data):\n",
    "    total_entries = 0\n",
    "\n",
    "    for class_title, class_data in data.items():\n",
    "        print(f\"\\n{class_title}\")\n",
    "        for section_title, entries in class_data.items():\n",
    "            print(f\"  {section_title} [Entries: {len(entries)}]\")\n",
    "            total_entries += len(entries)\n",
    "    print(f\"\\nTotal Entries: {total_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_structure(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have slightly over 1000 entries, which matches what we expected from the thesaurus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entry Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"\\n#1. Existence.—N. existence, being, entity, ens, esse,\\r\\nsubsistence.\\r\\n     reality, actuality; positiveness &c. adj.; fact, matter of fact, sober\\r\\nreality; truth &c. 494; actual existence.\\r\\n     presence &c. (existence in space) 186; coexistence &c. 120.\\r\\n     stubborn fact, hard fact; not a dream &c. 515; no joke.\\r\\n     center of life, essence, inmost nature, inner reality, vital\\r\\nprinciple.\\r\\n     [Science of existence], ontology.\\r\\n     V. exist, be; have being &c. n.; subsist, live, breathe, stand,\\r\\nobtain, be the case; occur &c. (event) 151; have place, prevail; find\\r\\noneself, pass the time, vegetate.\\r\\n     consist in, lie in; be comprised in, be contained in, be constituted\\r\\nby.\\r\\n     come into existence &c. n.; arise &c. (begin) 66; come forth &c.\\r\\n(appear) 446.\\r\\n     become &c. (be converted) 144; bring into existence &c. 161.\\r\\n     abide, continue, endure, last, remain, stay.\\r\\n     Adj. existing &c. v.; existent, under the sun; in existence &c. n.;\\r\\nextant; afloat, afoot, on foot, current, prevalent; undestroyed.\\r\\n     real, actual, positive, absolute; true &c. 494; substantial,\\r\\nsubstantive; self-existing, self-existent; essential.\\r\\n     well-founded,  well-grounded; unideal[obs3], unimagined; not potential\\r\\n&c. 2; authentic.\\r\\n     Adv. actually &c. adj.; in fact, in point of fact, in reality; indeed;\\nde facto, ipso facto.\\r\\n     Phr. ens rationis; ergo sum cogito; \\\"thinkest thou existence doth\\r\\ndepend on time?\\\" [Byron].\\r\\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning the Dataset & Extracting Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now have our dataset, but it's still quite useless for analysis as its format is not suitable for processing.\n",
    "\n",
    "* We will **clean the data** and **extract the synonyms** from the entries, so that we have a dataset ready for natural language pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's initialize a function that can extract phrases from entries. A few notable issues in entries are:\n",
    "\n",
    "\n",
    "    * Phrases are *usually* separated by commas, but also by semicolons and full stops.\n",
    "        * We will handle this by replacing semicolons and full stops with commas.\n",
    "\n",
    "\n",
    "    * There are a lot of special characters, line breaks and other formatting issues (e.g. `&c.`).\n",
    "        * We will handle this by replacing special characters with spaces and then removing any extra spaces.\n",
    "\n",
    "\n",
    "    * Phrases are not always separated by a space.\n",
    "        * We will handle this by replacing any double spaces with a single space.\n",
    "\n",
    "    \n",
    "    * There are words and phrases in brackets - which are sometimes relevant and sometimes not (e.g. `[obs3]` is not relevant).\n",
    "        * We will handle this by extracting any words or phrases in brackets and we will handle them later. This might lead to some useless phrases such as `obs`, but this is easily cleaned later on if deemed problematic - it is much more important to keep some relevant phrases that are in brackets.\n",
    "    \n",
    "\n",
    "    * We might end up with phrases that are 1 character long or less. \n",
    "        * These will be removed.\n",
    "\n",
    "\n",
    "\n",
    "* We will also create two functions for extracting Class and Section names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_entry(entry):\n",
    "    cleaned_phrases = []\n",
    "    \n",
    "    # Replace special characters with commas.\n",
    "    entry = re.sub(r\"[^A-Za-z'\\s]+\", \",\", entry)\n",
    "    entry_phrases = re.split(',', entry)\n",
    "    \n",
    "    for phrase in entry_phrases:\n",
    "        phrase = re.sub(\"[^A-Za-z'\\s]+\", \"\", phrase) \n",
    "        \n",
    "        cleaned_words = [word.lower() for word in phrase.split() if phrase] \n",
    "        cleaned_phrase = \" \".join(cleaned_words)\n",
    "        \n",
    "        # If the phrase is not empty, add it to the list of cleaned phrases.\n",
    "        if len(cleaned_phrase) > 1:\n",
    "            cleaned_phrases.append(cleaned_phrase)\n",
    "    \n",
    "    return cleaned_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_section(section_name):\n",
    "    section_name = section_name.lower()\n",
    "    relevant_name = re.sub(r\"[^A-Za-z\\s]+\", \"\", section_name)\n",
    "    relevant_name = \"_\".join(relevant_name.split()[2:])\n",
    "    relevant_name = re.sub(r\"\\s+\", \"_\", relevant_name)\n",
    "    \n",
    "    return \"sc_\" + relevant_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"existence\", \"space\", \"matter\", \"intellect\", \"volition\", \"affections\"]\n",
    "class_map = {raw_name: clean_name for raw_name, clean_name in zip(raw_data.keys(), class_names)}\n",
    "\n",
    "def sanitize_class(class_name):\n",
    "    return \"cl_\" + class_map[class_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since our classes are only six, we will manually create a list of their names.\n",
    "\n",
    "* Let's clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = {}\n",
    "\n",
    "for class_title, class_data in raw_data.items():\n",
    "    cleaned_class_data = {}\n",
    "    \n",
    "    for section_title, entries in class_data.items():\n",
    "        cleaned_entries = []\n",
    "        \n",
    "        for entry in entries:\n",
    "            cleaned_phrases = sanitize_entry(entry)\n",
    "            cleaned_entries.extend(cleaned_phrases)\n",
    "        \n",
    "        cleaned_section_name = sanitize_section(section_title)\n",
    "        cleaned_class_data[cleaned_section_name] = cleaned_entries\n",
    "    \n",
    "    cleaned_class_name = sanitize_class(class_title)\n",
    "    cleaned_data[cleaned_class_name] = cleaned_class_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a look at our data structure now that we have cleaned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_structure(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We notice that the amount of \"Entries\" is significantly higher, but now each entry was split into the phrases it contained.\n",
    "\n",
    "* We still might have irrelevant phrases, such as `obs`or other stopwords.\n",
    "\n",
    "* We will store our cleaned data in a new JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('thesaurus_cleaned.json', 'w') as json_file:\n",
    "    json.dump(cleaned_data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Who is JSON? (Dataset Restructuring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now have a scraped, categorised and cleaned dataset that may be used for a variety of tasks. Yay!\n",
    "\n",
    "* We have been working with JSON files as they are easy to read, write and parse in structured dictionary-like formats.\n",
    "\n",
    "* It is however important to note that we are going to be performing analytical and machine learning techniques, and JSON/dictionaries might not be the best formats for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will restructure our dataset into a pandas DataFrame, which is a more suitable format for data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = []\n",
    "section_names = []\n",
    "phrase_entry = []\n",
    "for class_name, sections in cleaned_data.items():\n",
    "    for section_name, phrases in sections.items():\n",
    "        for phrase in phrases:\n",
    "            class_names.append(class_name)\n",
    "            section_names.append(section_name)\n",
    "            phrase_entry.append(phrase)\n",
    "\n",
    "data_df = pd.DataFrame({'class': class_names, 'section': section_names, 'phrase': phrase_entry})\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will now perform whatever text preprocessing is left on our dataset to prepare it for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword and Non-English Phrase Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stopwords are words that are very common and carry little meaning, such as \"the\", \"and\", \"is\", \"in\", etc. They are usually removed from text when performing analytical tasks as they do not carry much contextual meaning.\n",
    "\n",
    "* Some words and phrases in our dataset are not in English, and we will remove them as well.\n",
    "\n",
    "* Some dataset-specific stopwords will also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_phrase(phrase, vocabulary, stopwords):\n",
    "    phrase = \" \".join([word for word in phrase.split() if word in vocabulary and word not in stopwords and len(word) > 1])\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\"obs\", \"adj\", \"adv\", \"lat\", \"fr\", \"ens\", \"de\", \"et\", \"al\", \"cf\", \"gr\"]\n",
    "stopwords = set(spacy_en.Defaults.stop_words).union(custom_stopwords)\n",
    "\n",
    "en_vocabulary_set = set(spacy_en.vocab.strings)\n",
    "\n",
    "data_df['phrase'] = data_df['phrase'].apply(sanitize_phrase, vocabulary=en_vocabulary_set, stopwords=stopwords)\n",
    "data_df = data_df[data_df['phrase'].apply(lambda x: len(x) > 0)].dropna(how='any').reset_index(drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that we removed almost half of our phrases, which is a good sign that we had a lot of irrelevant phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will encode our classes and sections into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_encoder = LabelEncoder()\n",
    "section_encoder = LabelEncoder()\n",
    "data_df['class_'] = class_encoder.fit_transform(data_df['class'])\n",
    "data_df['section_'] = section_encoder.fit_transform(data_df['section'])\n",
    "\n",
    "section_mappings = {section_encoding: section_label for section_label, section_encoding in zip(data_df['section'].unique(), data_df['section_'].unique())}\n",
    "class_mappings = {class_encoding: class_label for class_label, class_encoding in zip(data_df['class'].unique(), data_df['class_'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mappings, section_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a look at the class distribution of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = data_df['class'].value_counts(normalize=True).reset_index()\n",
    "class_distribution.columns = ['class', 'percentage']\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's significant class imbalance. This will be very apparent in clustering and classification tasks as well.\n",
    "\n",
    "* One technique that worked very well was removing phrases that appeared in more than half of the sections. This is a very high threshold, but it is a good way to remove phrases that are not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop_duplicates(subset='phrase').dropna(how='any').reset_index(drop=True)\n",
    "phrase_counts = data_df.groupby('phrase')['section'].nunique()\n",
    "phrases_to_remove = phrase_counts[phrase_counts >= data_df['section'].nunique()//2].index \n",
    "\n",
    "data_df = data_df[~data_df['phrase'].isin(phrases_to_remove)].dropna(how='any').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = data_df['class'].value_counts(normalize=True).reset_index()\n",
    "class_distribution.columns = ['class', 'percentage']\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Honorable Mentions\n",
    "\n",
    "* Lemmatization was implemented but it did not yield any significant improvements.\n",
    "\n",
    "* Other filtering techniques to remove irrelevant phrases were tested, but they did not yield any significant improvements or introduced significant bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have now finished our text preprocessing and we are ready to move on to the next step.\n",
    "\n",
    "* A large portion of our dataset was removed, but this is a good thing as it means we had a lot of irrelevant data.\n",
    "\n",
    "* There is no need for Tokenization as we will be using pre-trained word embeddings and these techniques are performed by their respective API's.\n",
    "\n",
    "* We will export our **clean** dataset as a CSV file for easy access and use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('thesaurus_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also extract a vocabulary of all the words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocabulary = data_df['phrase'].unique()\n",
    "len(dataset_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Embeddings\n",
    "\n",
    "You will embeddings for the word entries in Roget's Thesaurus. It is up to you to find the embeddings; you can use any of the available models. Older models like word2vec, GloVe, BERT, etc., may be easier to use, but recent models like Llama 2 and Mistral have been trained on larger corpora. OpenAI and Google offer their embeddings through APIs, but they are not free.\n",
    "\n",
    "You should think about how to store the embeddings you retrieve. You may use plain files (e.g., JSON, CSV) and vanilla Python, or a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voyage AI Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially, we will use the [Voyage AI](https://www.voyageai.com/) embeddings as they are (almost) free and easy to use. \n",
    "\n",
    "* A benefit of Voyage AI is that it automatically embeds phrases, which is useful for our dataset as it contains phrases and not just single words. \n",
    "\n",
    "* It also handles out-of-vocabulary words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_voyageai(vocabulary_array, batch_size=128):\n",
    "    unique_embeddings = {}\n",
    "    unique_vocabulary = np.unique(vocabulary_array).tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, len(unique_vocabulary), batch_size), desc=\"VoyageAI Processing\"):\n",
    "        batch_phrases = unique_vocabulary[i:i+batch_size]\n",
    "        batch_embeddings = voyage_client.embed(texts=batch_phrases, model=\"voyage-large-2\").embeddings\n",
    "        \n",
    "        for phrase, embedding in zip(batch_phrases, batch_embeddings):\n",
    "            unique_embeddings[phrase] = embedding\n",
    "            \n",
    "    return unique_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral AI Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For our second embedding model, we will use the [Mistral](https://mistral.ai/) embeddings.\n",
    "\n",
    "* Mistral is SOTA and has been trained on a large corpus, so it is a good choice for our task. \n",
    "\n",
    "* Just like Voyage, Mistral performs very well on well known benchmarks such as HuggingFace's [MTEB](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "* Mistral's embeddings are not free, but they are very cheap and we will only be using a small amount of them (arround 2 cents per run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_mistralai(vocabulary_array, batch_size=2048):\n",
    "    unique_embeddings = {}\n",
    "    unique_vocabulary = np.unique(vocabulary_array).tolist()\n",
    "        \n",
    "    for i in tqdm(range(0, len(unique_vocabulary), batch_size), desc=\"MistralAI Processing\"):\n",
    "        batch_phrases = unique_vocabulary[i:i+batch_size]\n",
    "        \n",
    "        embeddings_batch_response = mistral_client.embeddings(model=\"mistral-embed\", input=batch_phrases)\n",
    "        batch_embeddings = [e.embedding for e in embeddings_batch_response.data]\n",
    "        \n",
    "        for phrase, embedding in zip(batch_phrases, batch_embeddings):\n",
    "            unique_embeddings[phrase] = embedding\n",
    "            \n",
    "    return unique_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to save time and money, we will store the embeddings in a CSV file as it is convenient and easy to use. \n",
    "\n",
    "* Our embedding-fetching functions are smart - they only fetch once per phrase in our vocabulary, so we will not be making any repetitive API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(vocabulary_array):\n",
    "    # Extract embeddings\n",
    "    vygai_embeddings = extract_embeddings_voyageai(vocabulary_array)\n",
    "    mistral_embeddings = extract_embeddings_mistralai(vocabulary_array)\n",
    "    \n",
    "    # Combine embeddings and phrases into a DataFrame\n",
    "    embeddings_df = pd.DataFrame({'phrase': vocabulary_array, \n",
    "                                  'vygai_embedding': [vygai_embeddings[phrase] for phrase in vocabulary_array],\n",
    "                                  'mistralai_embedding': [mistral_embeddings[phrase] for phrase in vocabulary_array]})\n",
    "    \n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When fetching embeddings, we first want to check locally whether we have the embeddings already. If we do, we will retrieve the appropriate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_embeddings_to_df(data_df):\n",
    "    try:\n",
    "        embeddings_df = pd.read_csv('thesaurus_embeddings.csv', converters={'vygai_embedding': literal_eval, 'mistralai_embedding': literal_eval})\n",
    "    except FileNotFoundError:\n",
    "        embeddings_df = store_embeddings(data_df['phrase'].unique())\n",
    "        embeddings_df.to_csv('thesaurus_embeddings.csv', index=False)\n",
    "    \n",
    "    data_df = data_df.merge(embeddings_df, on=\"phrase\", how=\"left\")\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = attach_embeddings_to_df(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"VoyageAI Embedding Length: {len(data_df['vygai_embedding'].iloc[0])}\")\n",
    "print(f\"MistralAI Embedding Length: {len(data_df['mistralai_embedding'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our Voyage embeddings are 1536-dimensional, while our Mistral embeddings are 1024-dimensional.\n",
    "\n",
    "* Now that we have loaded them, lets ensure that we have no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.dropna(how='any')\n",
    "\n",
    "if data_df['vygai_embedding'].isna().sum() > 0:\n",
    "    print(f\"Missing VoyageAI Embeddings: {data_df['vygai_embedding'].isna().sum()}\")\n",
    "    \n",
    "if data_df['mistralai_embedding'].isna().sum() > 0:\n",
    "    print(f\"Missing MistralAI Embeddings: {data_df['mistralai_embedding'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exploring the embeddings is a good idea to understand how they represent our data. We will also see whether they cluster in any meaningful way.\n",
    "\n",
    "* Since our embeddings span across thousands of dimensions, we will need to reduce their dimensionality in order to visualize them. Luckily, we are well equipped for this.\n",
    "\n",
    "* We can also store these reductions in a column so as to save time in the future. \n",
    "\n",
    "* We will play around with a few different dimensionality reduction techniques to visualize our embeddings as a warmup for the next tasks.\n",
    "    * For every technique, we will create a _reducer function that will do the dirty work for us and simply return the dataframe-ready embeddings.\n",
    "\n",
    "* **NOTE**: For the sake of time and space, we will omit certain visualizations that are not very informative and have nothing new to offer. They have been filtered out during the creation of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's initially define a visualization function that will allow us to visualize our embeddings in 2 and 3 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(data_df, embedding_col_name, label_col_name, method=\"\", dimensions=2):\n",
    "    if dimensions == 2:\n",
    "        fig = px.scatter(data_df, x=data_df[embedding_col_name].apply(lambda x: x[0]), \n",
    "                         y=data_df[embedding_col_name].apply(lambda x: x[1]), \n",
    "                         color=data_df[label_col_name],\n",
    "                         color_discrete_sequence=px.colors.qualitative.Antique,\n",
    "                         labels={'x': 'Dimension 1', 'y': 'Dimension 2', 'color': label_col_name})\n",
    "        fig.update_traces(marker={'size': 3})\n",
    "    elif dimensions == 3:\n",
    "        fig = px.scatter_3d(data_df, \n",
    "                            x=data_df[embedding_col_name].apply(lambda x: x[0]), \n",
    "                            y=data_df[embedding_col_name].apply(lambda x: x[1]), \n",
    "                            z=data_df[embedding_col_name].apply(lambda x: x[2]), \n",
    "                            color=data_df[label_col_name], \n",
    "                            color_discrete_sequence=px.colors.qualitative.Antique, \n",
    "                            labels={'x': 'Dimension 1', 'y': 'Dimension 2', 'z': 'Dimension 3', 'color': label_col_name})\n",
    "        fig.update_traces(marker={'size': 2})\n",
    "        \n",
    "    fig.update_layout(title=f'Visualization of Phrases by {label_col_name} w/ {method}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis - A True Classic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start with PCA, which is a classic and simple dimensionality reduction technique.\n",
    "\n",
    "* PCA is unsupervised and linear, which means it is fast and easy to use.\n",
    "\n",
    "* We can easily measure the explained variance of our embeddings, which is a good indicator of how well they are represented in a lower dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reducer(embeddings, components=2):\n",
    "    embeddings_array = np.array(embeddings.tolist())\n",
    "    \n",
    "    pca = PCA(n_components=components, random_state=0)\n",
    "    reduced = pca.fit_transform(embeddings_array)\n",
    "    \n",
    "    return reduced.tolist(), pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA - VoyageAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start with 2D PCA for our VoyageAI embeddings. \n",
    "\n",
    "* We will also store the explained variance in a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"vygai_embedding_pca_2d\"], vygai_pca_2d_var = pca_reducer(data_df[\"vygai_embedding\"], components=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"vygai_embedding_pca_2d\", \"class\", method=\"PCA\", dimensions=2)\n",
    "print(f\"VoyageAI PCA 2D Variance Ratio: {vygai_pca_2d_var.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems that our embeddings are not very well represented in 2D, as we only capture approximately 6% of the variance. Τhis is problematic, but not unexpected, as we are dealing with high-dimensional data. \n",
    "\n",
    "* Our plot is not informative at all - let's try 3D PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"vygai_embedding_pca_3d\"], vygai_pca_3d_var = pca_reducer(data_df[\"vygai_embedding\"], components=3)\n",
    "\n",
    "visualize_embeddings(data_df, \"vygai_embedding_pca_3d\", \"class\", method=\"PCA\", dimensions=3)\n",
    "\n",
    "print(f\"VoyageAI PCA 3D Variance Ratio: {vygai_pca_3d_var.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results are not much better, as we only capture approximately 7.7% of the variance.\n",
    "\n",
    "* The plot is also not very informative, as the embeddings are still not well separated but we get the general direction of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's also plot by section for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"vygai_embedding_pca_2d\", \"section\", method=\"PCA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA - Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, we will perform 2D and 3D PCA for our Mistral embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"mistralai_embedding_pca_2d\"], mistralai_pca_2d_var = pca_reducer(data_df[\"mistralai_embedding\"], components=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"mistralai_embedding_pca_2d\", \"class\", method=\"PCA\", dimensions=2)\n",
    "print(f\"MistralAI PCA 2D Variance Ratio: {mistralai_pca_2d_var.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"mistralai_embedding_pca_3d\"], mistralai_pca_3d_var = pca_reducer(data_df[\"mistralai_embedding\"], components=3)\n",
    "\n",
    "visualize_embeddings(data_df, \"mistralai_embedding_pca_3d\", \"class\", method=\"PCA\", dimensions=3)\n",
    "\n",
    "print(f\"MistralAI PCA 3D Variance Ratio: {mistralai_pca_3d_var.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results are worse than VoyageAI, as we only capture approximately 4.5% and 5.7% of the variance respectively for 2D and 3D PCA.\n",
    "\n",
    "* The plots are also not expected to be very informative, as the embeddings are not well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * Let's also plot by section for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"mistralai_embedding_pca_2d\", \"section\", method=\"PCA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA - Conclusions and Dissapointment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Honestly, the results are not great. We can see that the explained variance is very low, which means that our embeddings are not well represented in a lower dimension.\n",
    "\n",
    "* We might be suffering from the **curse of dimensionality**, which is a common issue with high-dimensional data.\n",
    "\n",
    "* How many components do we need to explain most of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_ratio(benchmark, title):\n",
    "    cumulative_pca_result = np.cumsum(benchmark)\n",
    "    \n",
    "    n_components_for_90 = np.argmax(cumulative_pca_result >= 0.9) + 1\n",
    "    cumulative_pca_90 = cumulative_pca_result[n_components_for_90 - 1] if n_components_for_90 > 0 else 0\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Explained Variance per PC\", \"Cumulative Explained Variance\"))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(benchmark)+1)), y=benchmark, mode='lines+markers',\n",
    "                             name='PVE per PC'), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(benchmark)+1)), y=cumulative_pca_result, mode='lines+markers',\n",
    "                             name='Cumulative PVE', line=dict(color='orange')), row=1, col=2)\n",
    "\n",
    "    if n_components_for_90 > 0:\n",
    "        fig.add_shape(type='line', line=dict(dash='dash', color='red'),\n",
    "                      x0=n_components_for_90, x1=n_components_for_90, y0=0, y1=cumulative_pca_90,\n",
    "                      row=1, col=2, name=\"90% PVE\")\n",
    "        fig.add_trace(go.Scatter(x=[n_components_for_90], y=[cumulative_pca_90], text=[\"0.9\"], mode=\"text\", showlegend=False),\n",
    "                      row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Principal Component\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Principal Component\", row=1, col=2)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"PVE\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cumulative PVE\", range=[0, 1.05], row=1, col=2)\n",
    "\n",
    "    fig.update_layout(height=600, width=1000, title_text=title)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's try with 700 components and see if we can explain most of the variance.\n",
    "\n",
    "* This is a lot of components, but it is still less than the original dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 700\n",
    "\n",
    "voyage_pca_benchmark, voyage_pca_benchmark_var = pca_reducer(data_df['vygai_embedding'], components=N)\n",
    "mistral_pca_benchmark, mistral_pca_benchmark_var = pca_reducer(data_df['mistralai_embedding'], components=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's create a scree plot to visualize the explained variance.\n",
    "\n",
    "* The plot is interactive, so we can zoom in and see the explained variance for different numbers of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance_ratio(voyage_pca_benchmark_var, \"VoyageAI PCA Explained Variance Ratio\")\n",
    "plot_variance_ratio(mistral_pca_benchmark_var, \"MistralAI PCA Explained Variance Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that, in both cases, we need a lot of components to explain most of the variance, which is not ideal for visualization. Both need around **500-600 components** to explain 90% of the variance.\n",
    "    * MistralAI requires slightly less components than VoyageAI, but it is still a lot.\n",
    "\n",
    "* Let's play around with some other techniques and see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP - The New Kid on the Block\n",
    "\n",
    "* UMAP is a newer, non-linear and more advanced dimensionality reduction technique that is known for its ability to preserve local and global structure in high-dimensional data.\n",
    "\n",
    "* It's an advancement over t-SNE, which is another popular non-linear dimensionality reduction technique.\n",
    "\n",
    "* UMAP requires a lot of hyperparameter tuning and thus it is difficult to get the desired results.\n",
    "\n",
    "* We will only perform 2D UMAP, as 3D UMAP did not yield any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_reducer(embeddings, components=2):\n",
    "    embeddings_array = np.array(embeddings.tolist())\n",
    "\n",
    "    umap_ = umap.UMAP(n_components=components, metric='manhattan', init='random',  n_neighbors=50, min_dist=0.0) # manhattan as we dont need cosine similarity because the embeddings are already normalized\n",
    "    reduced = umap_.fit_transform(embeddings_array)\n",
    "    \n",
    "    return reduced.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP - VoyageAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"vygai_embedding_umap_2d\"] = umap_reducer(data_df[\"vygai_embedding\"], components=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"vygai_embedding_umap_2d\", \"class\", method=\"UMAP\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ... let's also plot by section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"vygai_embedding_umap_2d\", \"section\", method=\"UMAP\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP - MistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"mistralai_embedding_umap_2d\"] = umap_reducer(data_df[\"mistralai_embedding\"], components=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"mistralai_embedding_umap_2d\", \"class\", method=\"UMAP\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Section plotting omitted as it is not informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* UMAP does not seem to perform much better than PCA, as we still capture a very low amount of variance.\n",
    "\n",
    "* This might be an issue due to the low amount of components we are using, but it is still not ideal for visualization. We expect it to perform better than PCA in higher dimensions.\n",
    "\n",
    "* MistralAI's visualization was omitted for brevity, but they were almost identical to VoyageAI's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis (LDA) - Almost Cheating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far we have only used unsupervised techniques, but we can also employ supervised techniques for dimensionality reduction.\n",
    "\n",
    "* LDA is a supervised technique that is known for its ability to separate classes in high-dimensional data by finding the best projection that maximizes the distance between classes.\n",
    "\n",
    "* We will use LDA to reduce our embeddings' dimensions and see if we can get a better visualization than PCA and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_reducer(embeddings, labels, n=3):\n",
    "    embeddings_array = np.array(embeddings.tolist())\n",
    "    \n",
    "    lda = LDA(n_components=n)\n",
    "    lda_result = lda.fit_transform(embeddings_array, labels)\n",
    "    \n",
    "    return lda_result.tolist(), lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA - VoyageAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"vygai_embedding_lda_2d\"], vygai_lda_2d_var = lda_reducer(data_df[\"vygai_embedding\"], data_df[\"class\"], n=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"vygai_embedding_lda_2d\", \"class\", method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"vygai_embedding_lda_3d\"], vygai_lda_3d_var = lda_reducer(data_df[\"vygai_embedding\"], data_df[\"class\"], n=3)\n",
    "\n",
    "visualize_embeddings(data_df, \"vygai_embedding_lda_3d\", \"class\", method=\"LDA\", dimensions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ... let's also plot by section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"vygai_embedding_lda_2d\", \"section\", method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA - Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"mistralai_embedding_lda_2d\"], mistralai_lda_2d_var = lda_reducer(data_df[\"mistralai_embedding\"], data_df[\"class\"], n=2)\n",
    "\n",
    "visualize_embeddings(data_df, \"mistralai_embedding_lda_2d\", \"class\", method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"mistralai_embedding_lda_3d\"], mistralai_lda_3d_var = lda_reducer(data_df[\"mistralai_embedding\"], data_df[\"class\"], n=3)\n",
    "\n",
    "visualize_embeddings(data_df, \"mistralai_embedding_lda_3d\", \"class\", method=\"LDA\", dimensions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ... let's also plot by section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"mistralai_embedding_lda_2d\", \"section\", method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA - Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"3D LDA Explained VoyageAI Variance Ratio: {vygai_lda_3d_var.sum()}\")\n",
    "print(f\"3D LDA Mistral Explained Variance Ratio: {mistralai_lda_3d_var.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LDA performed very well on our embeddings, which is not surprising as it is a supervised technique. With just 3 components, we were able to capture around 70-75% of the variance.\n",
    "\n",
    "* We were finally able to get a sneak peek of our embeddings, and we can see that they are sort of seperated in this 3D space. By rotating the plot, we can see that the embeddings are not perfectly seperated, but they are not a complete mess either as they lean towards different directions.\n",
    "\n",
    "* It is important to note that LDA is a supervised technique, and it is not always possible to use it. It is also not always the best choice for dimensionality reduction, but it seems to have worked well for our embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Honorable Mentions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t-SNE is a very popular and powerful dimensionality reduction technique, but it is also very slow and not suitable for high-dimensional data.\n",
    "\n",
    "* In our case each run took over 10 minutes, which is not feasible for our task and not worth the time.\n",
    "\n",
    "* The results were also not great, as we can see that the embeddings are not well represented in a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "* We have explored a few different dimensionality reduction techniques and we have seen that our embeddings are not well represented in a lower dimension.\n",
    "\n",
    "* This is a common issue with high-dimensional data, and it is known as the curse of dimensionality. Additionally, we have *a lot* of data - something that will make visualization difficult anyway.\n",
    "\n",
    "* We are unlikely to enjoy any satisfying visualizations of our embeddings, but we can still use them for analytical and machine learning tasks.\n",
    "\n",
    "* We have prepared our visualization-friendly dimensionality-recued embeddings for the next steps, and we are ready to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "With the embeddings at hand, you can check whether unsupervised Machine Learning methods can arrive at classifications that are comparable to the Roget's Thesaurus Classification. You can use any clustering method of your choice (experiment freely). You must decide how to measure the agreement between the clusters you find and the classes defined by Roget's Thesaurus and report your results accordingly. The comparison will be at the class level (six classes) and the section / division level (so there must be two different clusterings, unless you can find good results with hierarchical clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we're focusing on checking agreement between clusters and labels, we will not use techniques such as the Elbow Method or Silhouette Score, as they are not really required for our task.\n",
    "\n",
    "* Instead, we will measure the ground truth labels against the clusters with the techniques that follow.\n",
    "\n",
    "* Additionally, we will cheat a bit as we know the expected number of clusters. We will use this information to try and replicate Roget's classification, although this is not applicable in real-world scenarios. Perhaps the student does not fall far from the professor's slides (where student = apple and professor's slides = tree).\n",
    "\n",
    "* A *note*; we will not visualize the clusters every single time as it will result in an extremely large amount of plots. We will only visualize the clusters we find interesting or important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_classes_n = len(data_df['class'].unique())\n",
    "expected_sections_n = len(data_df['section'].unique())\n",
    "\n",
    "print(f\"Expected Classes: {expected_classes_n}\")\n",
    "print(f\"Expected Sections: {expected_sections_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We define a function that creates pivot table that will allow us to compare our clusters with the Roget's Thesaurus classification.\n",
    "\n",
    "* With the pivot table, we can easily see how many words from each class and section were assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distribution(df_clustered, cluster_column, hue):\n",
    "    distr = df_clustered.groupby([cluster_column, hue]).size().unstack(fill_value=0)\n",
    "    distr['total'] = distr.sum(axis=1)\n",
    "    distr['winner'] = distr.iloc[:, :-1].idxmax(axis=1)\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Adjusted Rand Index (ARI)** - ARI is a measure of the similarity between two data clusterings. It considers all pairs of samples and counts pairs that are assigned in the same or different clusters in the predicted and true clusterings. The values range from -1 to 1, where 1 indicates perfect agreement and 0 indicates random agreement.\n",
    "\n",
    "* **Normalized Mutual Information (NMI)** - NMI is a measure of the agreement between two clusterings of the same data. It is normalized against chance. The values range from 0 to 1, where 1 indicates perfect agreement and 0 indicates random agreement.\n",
    "\n",
    "* **Homogeneity, Completeness and V-measure** - These are three related measures that can be used to evaluate the quality of clusters. Their values range from 0 to 1, where 1 indicates perfect agreement and 0 indicates random agreement.\n",
    "    * Homogeneity measures whether all of the clusters contain only data points which are members of a single class. \n",
    "    * Completeness measures whether all members of a given class are assigned to the same cluster. \n",
    "    * V-measure is the harmonic mean of homogeneity and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_agreement(true_labels, predictions, setup=\"\"):\n",
    "    predicted_labels = np.array(predictions)\n",
    "\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    homogeneity = homogeneity_score(true_labels, predicted_labels)\n",
    "    completeness = completeness_score(true_labels, predicted_labels)\n",
    "    v_measure = v_measure_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return pd.DataFrame({'setup': setup, 'ARI': ari, 'NMI': nmi, 'homogeneity': homogeneity, 'completeness': completeness, 'v-measure': v_measure}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use a copy of our original dataframe to perform these evaluations, as we will be modifying the dataframe in the process.\n",
    "\n",
    "* For each clustering technique, we will perform these evaluations and store the results in a dataframe for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start off with K-Means clustering, which is a simple and popular clustering technique.\n",
    "\n",
    "* K-Means suffers from the curse of dimensionality more than other techniques, but it is still worth to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans(embeddings, n_clusters):\n",
    "    if isinstance(embeddings, pd.Series):\n",
    "        embeddings = np.array(embeddings.tolist())\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "    predictions = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_metrics_df = pd.DataFrame(columns=['setup', 'ARI', 'NMI', 'homogeneity', 'completeness', 'v-measure'])\n",
    "kmeans_section_metrics_df = pd.DataFrame(columns=['setup', 'ARI', 'NMI', 'homogeneity', 'completeness', 'v-measure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Dimension Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start off by clustering our embeddings without any dimensionality reduction.\n",
    "\n",
    "* We do not expect great results, but it is a good experiment to see how K-Means performs on high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_default_vygai_setup = \"vygai_kmeans_class_default\"\n",
    "\n",
    "cluster_data_df[kmeans_class_default_vygai_setup] = perform_kmeans(cluster_data_df['vygai_embedding'], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], cluster_data_df[kmeans_class_default_vygai_setup], \n",
    "                                                                                setup=kmeans_class_default_vygai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_default_vygai_setup, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_default_mistralai_setup = \"mistralai_kmeans_class_default\"\n",
    "\n",
    "cluster_data_df[kmeans_class_default_mistralai_setup] = perform_kmeans(cluster_data_df['mistralai_embedding'], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], cluster_data_df[kmeans_class_default_mistralai_setup], \n",
    "                                                                                setup=kmeans_class_default_mistralai_setup)], ignore_index=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df,kmeans_class_default_mistralai_setup, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The distribution of words in the clusters is not very good, as we can see that the words are not well separated. This was expected due to the high dimensionality of our data.\n",
    "\n",
    "* In fact, for both our embeddings, `cl_volition` strongly dominates the clusters, which is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will now use PCA to reduce the dimensions of our embeddings and then perform K-Means clustering.\n",
    "\n",
    "* We will use 550 components for VoyageAI and 510 components for MistralAI, as these are the number of components required to explain around 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vygai_pca_emb_optimal = pca_reducer(cluster_data_df['vygai_embedding'], components=550)\n",
    "mistralai_pca_emb_optimal = pca_reducer(cluster_data_df['mistralai_embedding'], components=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"VoyageAI PCA Optimal Variance Ratio: {vygai_pca_emb_optimal[1].sum()}\")\n",
    "print(f\"MistralAI PCA Optimal Variance Ratio: {mistralai_pca_emb_optimal[1].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_pca_vygai_setup = \"vygai_kmeans_class_pca\"\n",
    "\n",
    "cluster_data_df[kmeans_class_pca_vygai_setup] = perform_kmeans(vygai_pca_emb_optimal[0], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], \n",
    "                                                                                cluster_data_df[kmeans_class_pca_vygai_setup],\n",
    "                                                                                setup=kmeans_class_pca_vygai_setup)\n",
    "                                     ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_pca_vygai_setup, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_pca_mistralai_setup = \"mistralai_kmeans_class_pca\"\n",
    "\n",
    "cluster_data_df[kmeans_class_pca_mistralai_setup] = perform_kmeans(mistralai_pca_emb_optimal[0], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], cluster_data_df[kmeans_class_pca_mistralai_setup],\n",
    "                                                                                setup=kmeans_class_pca_mistralai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_pca_mistralai_setup, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can still see that the words are not well separated in the expected clusters - as there is no distinct category dominating in words for each cluster.\n",
    "\n",
    "* Let's take a look at the 2D PCA plot for VoyageAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(cluster_data_df, \"vygai_embedding_pca_2d\", kmeans_class_pca_vygai_setup, method=\"KMeans\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The plot shows that the kmeans algorithm *thinks* that it is clustering in an acceptable manner, however the real distribution is completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UMAP & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, we will use UMAP to reduce the dimensions of our embeddings and then perform K-Means clustering.\n",
    "\n",
    "* We will use 2 components for VoyageAI and MistralAI as it will make no difference whatsoever in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_umap_vygai_setup = \"vygai_kmeans_class_umap\"\n",
    "\n",
    "cluster_data_df[kmeans_class_umap_vygai_setup] = perform_kmeans(data_df[\"vygai_embedding_umap_2d\"], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], cluster_data_df[kmeans_class_umap_vygai_setup],\n",
    "                                                                                setup=kmeans_class_umap_vygai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_umap_vygai_setup, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_umap_mistralai_setup = \"mistralai_kmeans_class_umap\"\n",
    "\n",
    "cluster_data_df[kmeans_class_umap_mistralai_setup] = perform_kmeans(data_df[\"mistralai_embedding_umap_2d\"], n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], cluster_data_df[kmeans_class_umap_mistralai_setup],\n",
    "                                                                                setup=kmeans_class_umap_mistralai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_umap_mistralai_setup, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We still see that the words are not well separated in the expected clusters - as there is no distinct category for each cluster.\n",
    "\n",
    "* Let's take a look at the 2D UMAP plot for VoyageAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(cluster_data_df, \"vygai_embedding_umap_2d\", kmeans_class_umap_vygai_setup, method=\"UMAP\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It looks like a nice kite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_metrics_df = kmeans_class_metrics_df.sort_values(by='setup', ascending=False, ignore_index=True)\n",
    "kmeans_class_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we see, K-Means clustering did not perform well on our high-dimensional data despite our best efforts to reduce the dimensions.\n",
    "\n",
    "* We can see that the words are not well separated in the clusters, and there is no distinct category for each cluster.\n",
    "\n",
    "* We will now move on to section / division level clustering with our heads high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_df['vygai_kmeans_class_pca'] = cluster_data_df['vygai_kmeans_class_pca'].astype(str)\n",
    "visualize_embeddings(cluster_data_df, \"vygai_embedding_pca_2d\", \"vygai_kmeans_class_pca\", method=\"PCA for clustering labels\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(cluster_data_df, \"vygai_embedding_pca_2d\", \"class\", method=\"PCA for actual labels\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will also perform clustering at the section/division level, as this is the second level of hierarchy in Roget's Thesaurus.\n",
    "    * Reminder: We name our section/division level as \"sc_\" for simplicity, irregardless of type.\n",
    "\n",
    "* For brevity, we will only perform the clustering with all dimensions and PCA-reduced embeddings as we have seen that UMAP does not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Dimension Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_default_vygai_setup = \"vygai_kmeans_section_default\"\n",
    "\n",
    "cluster_data_df[kmeans_section_default_vygai_setup] = perform_kmeans(cluster_data_df['vygai_embedding'], n_clusters=expected_sections_n)\n",
    "\n",
    "kmeans_section_metrics_df = pd.concat([kmeans_section_metrics_df, measure_agreement(cluster_data_df['section_'], cluster_data_df[kmeans_section_default_vygai_setup],\n",
    "                                                                                setup=kmeans_section_default_vygai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_section_default_vygai_setup, 'section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_default_mistralai_setup = \"mistralai_kmeans_section_default\"\n",
    "\n",
    "cluster_data_df[kmeans_section_default_mistralai_setup] = perform_kmeans(cluster_data_df['mistralai_embedding'], n_clusters=expected_sections_n)\n",
    "\n",
    "kmeans_section_metrics_df = pd.concat([kmeans_section_metrics_df, measure_agreement(cluster_data_df['section_'], cluster_data_df[kmeans_section_default_mistralai_setup],\n",
    "                                                                                setup=kmeans_section_default_mistralai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_section_default_mistralai_setup, 'section')[['total', 'winner']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_pca_vygai_setup = \"vygai_kmeans_section_pca\"\n",
    "\n",
    "cluster_data_df[kmeans_section_pca_vygai_setup] = perform_kmeans(vygai_pca_emb_optimal[0], n_clusters=expected_sections_n)\n",
    "\n",
    "kmeans_section_metrics_df = pd.concat([kmeans_section_metrics_df, measure_agreement(cluster_data_df['section_'],\n",
    "                                                                                cluster_data_df[kmeans_section_pca_vygai_setup],\n",
    "                                                                                setup=kmeans_section_pca_vygai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_section_pca_vygai_setup, 'section')[['total', 'winner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_pca_mistralai_setup = \"mistralai_kmeans_section_pca\"\n",
    "\n",
    "cluster_data_df[kmeans_section_pca_mistralai_setup] = perform_kmeans(mistralai_pca_emb_optimal[0], n_clusters=expected_sections_n)\n",
    "\n",
    "kmeans_section_metrics_df = pd.concat([kmeans_section_metrics_df, measure_agreement(cluster_data_df['section_'],\n",
    "                                                                                cluster_data_df[kmeans_section_pca_mistralai_setup],\n",
    "                                                                                setup=kmeans_section_pca_mistralai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_section_pca_mistralai_setup, 'section')[['total', 'winner']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_metrics_df = kmeans_section_metrics_df.sort_values(by='setup', ascending=False, ignore_index=True)\n",
    "kmeans_section_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that still the results are not great. There are sections dominating the clusters, and the words are not well separated in the clusters. \n",
    "\n",
    "* VoyageAI's results are slightly better than MistralAI's, but they are still not in the realm of good.\n",
    "\n",
    "* Let's visualize the clusters against the actual labels for the sake of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_df['vygai_kmeans_section_pca'] = cluster_data_df['vygai_kmeans_section_pca'].astype(str)\n",
    "visualize_embeddings(cluster_data_df, \"vygai_embedding_pca_2d\", \"vygai_kmeans_section_pca\", method=\"PCA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(data_df, \"vygai_embedding_pca_2d\", \"section\", method=\"PCA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Honorable Mentions (that I attempted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following clustering techniques were tested but omitted so as to limit the length of the notebook and save run time. Their results were either worse than K-Means or they never finished running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **HDBSCAN** - HDBSCAN is a density-based clustering technique that is known for its ability to find clusters of varying density. While attempting to use it, we found that it was not suitable for our task as it was taking too long.\n",
    "\n",
    "* **Gaussian Mixture Models** - GMM is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions. We attempted to use it, but it was not suitable for our task as it was taking too long. Running it for all dimensions is impossible, running it on our reduced embeddings seems pointless as they are not isotropic.\n",
    "\n",
    "* **Spectral Clustering** - Spectral clustering is a technique that uses the eigenvalues of a similarity matrix to reduce the dimensionality of the data before clustering in a lower dimensional space. Apart from being slow, it was not suitable for our task as it gave us a warning about the graph not being fully connected and the results were worse than random. Running it for all dimensions is impossible, running it on our reduced embeddings seems pointless as the data is not in the proper distribution for it to work well.\n",
    "\n",
    "* **Hierarchical Clustering** - Hierarchical clustering is a technique that builds a hierarchy of clusters. Its runtime was too long and the results too disappointing to be included in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Cheating Swan Song featuring Linear Discriminant Analysis & K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we saw earlier, LDAs performed very well on our embeddings, which is not surprising as it is a supervised technique. With just 3 components, we were able to capture around 70-75% of the variance.\n",
    "\n",
    "* Using its results completely defeats the purpose of our task, as we are trying to find clusters without any prior knowledge of the data.\n",
    "\n",
    "* However I could not help but try as I find it fascinating.\n",
    "\n",
    "* For brevity, we will only perform it on the MistralAI embeddings.\n",
    "\n",
    "* We will re-perform LDA for the maximum amount of components (# of classes - 1) and then perform K-Means clustering.\n",
    "\n",
    "* We will write our conclusions in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_lda_mistralai_setup = \"mistralai_kmeans_class_lda\"\n",
    "\n",
    "lda_optimal_class, _ = lda_reducer(cluster_data_df['mistralai_embedding'], cluster_data_df['class'], n=expected_classes_n-1)\n",
    "\n",
    "cluster_data_df[kmeans_class_lda_mistralai_setup] = perform_kmeans(lda_optimal_class, n_clusters=expected_classes_n)\n",
    "\n",
    "kmeans_class_metrics_df = pd.concat([kmeans_class_metrics_df, measure_agreement(cluster_data_df['class_'], \n",
    "                                                                                cluster_data_df[kmeans_class_lda_mistralai_setup],\n",
    "                                                                                setup=kmeans_class_lda_mistralai_setup)\n",
    "                                     ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_df['mistralai_kmeans_class_lda'] = cluster_data_df['mistralai_kmeans_class_lda'].astype(str)\n",
    "visualize_embeddings(cluster_data_df, \"mistralai_embedding_lda_2d\", kmeans_class_lda_mistralai_setup, method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_class_lda_mistralai_setup, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The seperation is not perfect, but it is much better than what we have seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_class_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The numerical measures are also much better than what we have seen so far compared to the other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_lda_mistralai_setup = \"mistralai_kmeans_section_lda\"\n",
    "\n",
    "lda_optimal_section, _ = lda_reducer(cluster_data_df['mistralai_embedding'], cluster_data_df['section_'], n=expected_sections_n-1)\n",
    "\n",
    "cluster_data_df[kmeans_section_lda_mistralai_setup] = perform_kmeans(lda_optimal_section, n_clusters=expected_sections_n)\n",
    "\n",
    "kmeans_section_metrics_df = pd.concat([kmeans_section_metrics_df, measure_agreement(cluster_data_df['section_'],\n",
    "                                                                                cluster_data_df[kmeans_section_lda_mistralai_setup],\n",
    "                                                                                setup=kmeans_section_lda_mistralai_setup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_df['mistralai_kmeans_section_lda'] = cluster_data_df['mistralai_kmeans_section_lda'].astype(str)\n",
    "visualize_embeddings(cluster_data_df, \"mistralai_embedding_lda_2d\", kmeans_section_lda_mistralai_setup, method=\"LDA\", dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distribution(cluster_data_df, kmeans_section_lda_mistralai_setup, 'section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_section_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Likewise, the seperation is not perfect, but it is much better than what we have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reducing the dimensions of our embeddings with LDA and then performing K-Means clustering gave us the best results so far.\n",
    "\n",
    "* Is this cheating? Yes, it is. But it is also a good experiment to see how well our embeddings can be seperated with prior knowledge. \n",
    "\n",
    "* Why is it cheating? Because we are clustering on a biased subspace of our data that have been selected to maximize the seperation of the classes.\n",
    "\n",
    "* It was still fun to play with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clustering did not yield satisfactory results - it seems that unsupervised methods cannot arrive at classifications that are comparable to the Roget's Thesaurus Classification, at least with the embeddings we have.\n",
    "\n",
    "* This might be because the thesaurus was published in 1852 while our embeddings are derived from internet corpora, which are much more recent. Language changes over time, and the meaning of words changes with it. A lot of the words in the thesaurus are quite uncommon and archaic, and they might not be well represented in our embeddings. \n",
    "\n",
    "* Due to the high dimensionality of our embeddings, we were unable to find any meaningful clusters. We attempted to reduce the dimensions with PCA, UMAP and (cheekily) LDA, but the results were still not satisfactory. Both simple clustering algorithms such as k-means and more sophisticated ones such as HDBSCAN and GMM did not yield any meaningful results.\n",
    "\n",
    "* The aforementioned apply to both class and section level clustering, although the results were slightly better for the section level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction\n",
    "\n",
    "Now we flip over to supervised Machine Learning methods. You must experiment and come up with the best classification method, whose input will be a word and its target will be its class, or its section / division (so there must be two different models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will attempt to predict the class and section of a word using our embeddings with supervised machine learning methods.\n",
    "\n",
    "* A **note**; in order to avoid repetition, we will only use the embeddings that yielded the best results for each technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(supervised_df['mistralai_embedding'].tolist(), supervised_df['class'].tolist(), test_size=0.2, random_state=0)\n",
    "X_train_section, X_test_section, Y_train_section, Y_test_section = train_test_split(supervised_df['mistralai_embedding'].tolist(), supervised_df['section'].tolist(), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will define any necessary functions for evaluation plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix with Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_confusion_matrix(true_labels, predicted_labels, title):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially, we will use a dummy classifier as a baseline to compare our models against.\n",
    "\n",
    "* We will use the stratified strategy to ensure that the class distribution is preserved.\n",
    "\n",
    "* We will also use the VoyageAI embeddings - without a specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's start with the class classification baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_class = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy_class.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_class_predictions = dummy_class.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, dummy_class_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dummy classifier, as expected, performs as if it is random (6 classes = 16% accuracy if random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_section = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy_section.fit(X_train_section, Y_train_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_section_predictions = dummy_section.predict(X_test_section)\n",
    "\n",
    "print(classification_report(Y_test_section, dummy_section_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our baseline is set! Let's move on to the real models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start off with a Naive Bayes classifier, which is a simple and popular classification technique for text data.\n",
    "\n",
    "* We will use the Gaussian Naive Bayes classifier, as it is suitable for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We start off by fitting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can then predict on our test set and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_gnb_class = gnb.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred_gnb_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's also visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_confusion_matrix(Y_test, Y_pred_gnb_class, \"Gaussian Naive Bayes Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_section = GaussianNB()\n",
    "gnb_section.fit(X_train_section, Y_train_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_gnb_section = gnb_section.predict(X_test_section)\n",
    "print(classification_report(Y_test_section, Y_pred_gnb_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes seems to not perform very well despite being almost 3 times better than the dummy classifier. \n",
    "\n",
    "* Our data is extremely high-dimensional and a simple probabilistic model such as Naive Bayes is perhaps not the best choice for our task.\n",
    "\n",
    "* Still, we have outperformed our baseline and it sets a new standard for us to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Classifier feat. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM is a true classic and a powerful classification technique that is known for its ability to find the best hyperplane that separates the classes.\n",
    "\n",
    "* SVM does not perform well on high-dimensional data (takes too long), we will use LDA to reduce the dimensions.\n",
    "\n",
    "* For SVM, VoyageAI's embeddings will be used for diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we want to split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class_svm, X_test_class_svm, Y_train_class_svm, Y_test_class_svm = train_test_split(supervised_df[\"vygai_embedding\"].tolist(), supervised_df['class'].tolist(), test_size=0.2, random_state=0)\n",
    "X_train_section_svm, X_test_section_svm, Y_train_section_svm, Y_test_section_svm = train_test_split(supervised_df[\"vygai_embedding\"].tolist(), supervised_df['section'].tolist(), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we said, we'll use LDA to reduce the dimensions of our embeddings. \n",
    "\n",
    "* In order to avoid test set leakage, we will fit the LDA on the training set and then transform both the training and testing sets. \n",
    "\n",
    "* We will use the maximum amount of components that LDA can provide us with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_svm = LDA(n_components=expected_classes_n - 1)\n",
    "X_train_class_svm = lda_svm.fit_transform(X_train_class_svm, Y_train_class_svm)\n",
    "X_test_class_svm = lda_svm.transform(X_test_class_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', class_weight='balanced')\n",
    "svm.fit(X_train_class_svm, Y_train_class_svm) \n",
    "Y_pred_class_svm = svm.predict(X_test_class_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test_class_svm, Y_pred_class_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems that our model is not performing very well, but we are still going upwards! \n",
    "\n",
    "* In fact, the LDA trick performs much better than other dimensionality reduction techniques as tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_confusion_matrix(Y_test_class_svm, Y_pred_class_svm, \"SVM Class Classification Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Likewise, we'll train our SVM model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_svm = LDA(n_components=expected_sections_n - 1)\n",
    "X_train_section_svm = lda_svm.fit_transform(X_train_section_svm, Y_train_section_svm)\n",
    "X_test_section_svm = lda_svm.transform(X_test_section_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "svm.fit(X_train_section_svm, Y_train_section_svm)\n",
    "Y_pred_section_svm = svm.predict(X_test_section_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test_section_svm, Y_pred_section_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot the predictions on the PCA reduced embeddings\n",
    "fig = px.scatter(x=[x[0] for x in X_test_class_svm], y=[x[1] for x in X_test_class_svm], color=Y_pred_class_svm, color_discrete_sequence=px.colors.qualitative.Antique)\n",
    "fig.update_layout(title=\"SVM Class Classification on PCA Reduced VoyageAI Embeddings\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM was an improvement over anything we have seen so far, but we are still yet to reach the a satisfactory level of performance.\n",
    "\n",
    "* LDA was of great help both accuracy and performance-wise, and it seems that it is the best choice for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo Linear Discriminant Analysis (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LDA performed very well on our embeddings, which is not surprising as it is a supervised technique. With just 3 components, we were able to capture around 70-75% of the variance. Let's see how it performs in a classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vyg_lda_class, X_test_vyg_lda_class, Y_train_vyg_lda_class, Y_test_vyg_lda_class = train_test_split(supervised_df[\"mistralai_embedding\"].tolist(), supervised_df['class'].tolist(), test_size=0.2, random_state=0)\n",
    "X_train_vyg_lda_section, X_test_vyg_lda_section, Y_train_vyg_lda_section, Y_test_vyg_lda_section = train_test_split(supervised_df[\"mistralai_embedding\"].tolist(), supervised_df['section'].tolist(), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=expected_classes_n - 1)\n",
    "lda.fit(X_train_vyg_lda_class, Y_train_vyg_lda_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lda_class = lda.predict(X_test_vyg_lda_class)\n",
    "print(classification_report(Y_test_vyg_lda_class, Y_pred_lda_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=expected_sections_n - 1)\n",
    "lda.fit(X_train_vyg_lda_section, Y_train_vyg_lda_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lda_section = lda.predict(X_test_vyg_lda_section)\n",
    "print(classification_report(Y_test_vyg_lda_section, Y_pred_lda_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Class-level classification seems almost identical to SVM, but section-level classification has improved slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's move onto more sophisticated models. We will use a simple neural network for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to promote code reusability, we will define a few helper functions that will allow us to easily create and evaluate our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we'll define the data splitting function.\n",
    "\n",
    "* We want a training, validation and testing set. \n",
    "\n",
    "* We also want to give the option for resampling, as our dataset is imbalanced. (A lot of experimentation was done with this, but it did not yield any significant improvements.)\n",
    "    * It is important to resample **only** the training set, as we do not want to introduce bias in our validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "def prepare_dataset(df, embedding_col_name, label_col_name, test_size=0.2, val_size=0.2, enable_resampling=False, random_state=0):\n",
    "    X = np.stack(df[embedding_col_name].values)\n",
    "    Y = df[label_col_name].values\n",
    "    \n",
    "    Y_categorical = to_categorical(Y)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y_categorical, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=val_size)\n",
    "    \n",
    "    if enable_resampling:\n",
    "        oversampler = SMOTE(sampling_strategy='not majority')\n",
    "        undersampler = InstanceHardnessThreshold(random_state=0,estimator=GaussianNB(), sampling_strategy='majority', cv=10)\n",
    "        pipeline = Pipeline([('undersampling', undersampler), ('oversampling', oversampler)])\n",
    "        X_train, Y_train = pipeline.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we'll define our trainer and evaluator functions.\n",
    "\n",
    "* We will use the Adam optimizer, as it is a good choice for most tasks. \n",
    "\n",
    "* We will use the sparse categorical focal crossentropy loss function, as it is suitable for *unbalanced* multi-class classification tasks.\n",
    "\n",
    "* Finally, we'll use callback functions to stop training if the validation loss does not improve for a certain amount of epochs. Specifically:\n",
    "    * EarlyStopping will stop training if the validation loss does not improve for a few epochs.\n",
    "    * ReduceLROnPlateau will reduce the learning rate if the validation loss does not improve for a few epochs.\n",
    "    * LearningRateScheduler will reduce the learning rate after a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, X_train, Y_train, X_val, Y_val, epochs=100, batch_size=32, learning_rate=0.001, verbose=0):\n",
    "    step_decay = lambda epoch: 0.001 * 0.2**(np.floor(epoch / 10))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=CategoricalFocalCrossentropy(), metrics=['accuracy', 'AUC'])\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=0.00001),\n",
    "        LearningRateScheduler(step_decay)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        epochs=epochs,  \n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks, \n",
    "        verbose=verbose, \n",
    "        shuffle=True, \n",
    "        use_multiprocessing=True,\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy, auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return loss, accuracy, auc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And their respective plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Accuracy\"))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss'])+1)), y=history.history['loss'], mode='lines+markers', name='Training Loss'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss'])+1)), y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['accuracy'])+1)), y=history.history['accuracy'], mode='lines+markers', name='Training Accuracy'), row=1, col=2)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_accuracy'])+1)), y=history.history['val_accuracy'], mode='lines+markers', name='Validation Accuracy'), row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=1) \n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=2) \n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(title_text=title, height=500, width=1000)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are now ready to train our neural network for class-level prediction.\n",
    "\n",
    "* We will build a two-layer neural network, with 64 neurons in the first layer and 32 neurons in the second layer. \n",
    "    * The number of neurons was chosen through experimentation with validation data.\n",
    "    * We will use L1 regularization to pick up on the most important features and prevent overfitting.\n",
    "* We will also use the PReLU activation function, as it is a good choice for most tasks and an improvement over ReLU.\n",
    "* Each batch will be normalized so as to improve convergence.\n",
    "* Finally, Dropout will be used to prevent overfitting and improve generalization as it acts like an ensemble of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn_class, X_val_nn_class, X_test_nn_class, Y_train_nn_class, Y_val_nn_class, Y_test_nn_class = prepare_dataset(supervised_df, 'mistralai_embedding', 'class_', test_size=0.2, val_size=0.2, random_state=0)\n",
    "\n",
    "nn_class_mistralai = Sequential([ \n",
    "    Input(shape=(X_train_nn_class.shape[1],)),\n",
    "    Dense(64, kernel_regularizer=l1(1e-5)),\n",
    "    BatchNormalization(),\n",
    "    PReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, kernel_regularizer=l1(1e-5)),\n",
    "    BatchNormalization(),\n",
    "    PReLU(),\n",
    "    Dropout(0.5),    \n",
    "    Dense(Y_train_nn_class.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a look at our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_class_mistralai.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we'll plot our training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_nn_class_mistralai = trainer(nn_class_mistralai, X_train_nn_class, Y_train_nn_class, X_val_nn_class, Y_val_nn_class, epochs=150, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "plot_training_history(history_nn_class_mistralai, \"Neural Network Training History for MistralAI Class Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's see how it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_nn_class_mistralai, accuracy_nn_class_mistralai, auc_nn_class_mistralai = evaluate_model(nn_class_mistralai, X_test_nn_class, Y_test_nn_class)\n",
    "print(f\"Neural Network MistralAI Class Classification Loss: {loss_nn_class_mistralai}\")\n",
    "print(f\"Neural Network MistralAI Class Classification Accuracy: {accuracy_nn_class_mistralai}\")\n",
    "print(f\"Neural Network MistralAI Class Classification AUC: {auc_nn_class_mistralai}\")\n",
    "\n",
    "heatmap_confusion_matrix(np.argmax(Y_test_nn_class, axis=1), np.argmax(nn_class_mistralai.predict(X_test_nn_class), axis=1), \"Neural Network MistralAI Class Classification Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's repeat the process for section-level prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn_section, X_val_nn_section, X_test_nn_section, Y_train_nn_section, Y_val_nn_section, Y_test_nn_section = prepare_dataset(supervised_df, 'mistralai_embedding', 'section_', test_size=0.2, val_size=0.2, random_state=0) \n",
    "\n",
    "nn_section_mistralai = Sequential([\n",
    "    Input(shape=(X_train_nn_section.shape[1],)),\n",
    "    Dense(64, kernel_regularizer=l1(1e-5)),\n",
    "    BatchNormalization(),\n",
    "    PReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, kernel_regularizer=l1(1e-5)),\n",
    "    BatchNormalization(),\n",
    "    PReLU(),\n",
    "    Dropout(0.5),    \n",
    "    Dense(Y_train_nn_section.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_nn_section_mistralai = trainer(nn_section_mistralai, X_train_nn_section, Y_train_nn_section, X_val_nn_section, Y_val_nn_section, epochs=150, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "plot_training_history(history_nn_section_mistralai, \"Neural Network Training History for MistralAI Section Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_nn_section_mistralai, accuracy_nn_section_mistralai, auc_nn_section_mistralai = evaluate_model(nn_section_mistralai, X_test_nn_section, Y_test_nn_section)\n",
    "\n",
    "print(f\"Neural Network MistralAI Section Classification Loss: {loss_nn_section_mistralai}\")\n",
    "print(f\"Neural Network MistralAI Section Classification Accuracy: {accuracy_nn_section_mistralai}\")\n",
    "print(f\"Neural Network MistralAI Section Classification AUC: {auc_nn_section_mistralai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our neural network performed very average - and honestly not better than LDA-injected SVM or LDA.\n",
    "\n",
    "* Sadly, more complex networks reached overfitting and did not yield any significant improvements. Simpler networks did not perform any better than our last baseline.\n",
    "    * We could start throwing CNNs and RNNs at the problem, but it would not result in any significant improvements.\n",
    "\n",
    "* Despite applying a few advanced techniques such as PReLU and focal loss, we were unable to achieve any significant improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions & Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Roget's Thesaurus was a challenging task to classify, and despite the disappointing results, a lot was learned in experimenting with different techniques.\n",
    "\n",
    "\n",
    "* Due to the old age of the thesaurus, part of the language used in it is quite peculiar and maybe was not well represented in our embeddings - also suffering from the curse of dimensionality.\n",
    "    * This is especially noticeable when we try to cluster the words and predict their classes/sections.\n",
    "\n",
    "\n",
    "\n",
    "* In retrospect, using SOTA embeddings of such high dimensionality was not the best choice for our task. A tamer set of embeddings might have improved performance issues and allowed for more sophisticated techniques to be used. \n",
    "\n",
    "\n",
    "* Similarly, supervised classification techniques did not perform well at all. We quickly reached a ceiling and were unable to achieve any significant improvements between primitive and advanced models. \n",
    "\n",
    "\n",
    "* The aforementioned hint towards the following;\n",
    "    * The human-made classes and sections in the century-old Roget's Thesaurus are not well represented in our embedding space.\n",
    "    * There was an issue with the quality of the data preprocessing techniques applied to the dataset. In my defense, a lot of different methods were tested and the best ones were chosen. \n",
    "    * We did not have the resources to try out more sophisticated techniques such as pre-trained language models, which might have performed better - although it is not guaranteed due to the ceiling.\n",
    "    * Flattening the sections into divisions might have introduced some bias and created over-represented sections, which might have affected our results. The dataset was imbalanced on a class-level as well, with resampling techniques not yielding any significant improvements.\n",
    "\n",
    "* It would be very easy to fall into the trap of introducing bias in our models without realizing, such as performing SMOTE on *both* the training, validation and training sets, or fitting LDA on the entire dataset. It is important to be aware of these issues and to avoid them as much as possible, as they can lead to misleading results.\n",
    "\n",
    "* I, as a student, really enjoyed working with LDA and trying to think up of ways to utilize it in a classification task - as seen in SVM.\n",
    "    * I also had to scrap more than half of the notebook before delivery as it was too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can be made using the following function, simply replace the string with the phrase/word you want to classify, and specify whether you want to predict the class or section. Specifically, we will allow the reader to predict using our Neural Network model with the MistralAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, phrase) -> np.ndarray:\n",
    "    \n",
    "    embeddings_batch_response = mistral_client.embeddings(model=\"mistral-embed\", input=[phrase])\n",
    "    phrase_embedding = [np.array(e.embedding) for e in embeddings_batch_response.data][0]\n",
    "    phrase_embedding = np.array(phrase_embedding).reshape(1, -1)\n",
    "        \n",
    "    predictions = model.predict(phrase_embedding)\n",
    "    y_pred = np.argmax(predictions, axis=1)    \n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "class_prediction_model = nn_class_mistralai\n",
    "section_prediction_model = nn_section_mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_phrase = \"I love LDA\" \n",
    "\n",
    "class_pred = make_predictions(class_prediction_model, class_phrase)\n",
    "\n",
    "pred_class_label = class_encoder.inverse_transform(class_pred)\n",
    "print(f\"Predicted Class: {pred_class_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_phrase = \"SVM too\"\n",
    "\n",
    "section_pred = make_predictions(section_prediction_model, section_phrase)\n",
    "\n",
    "pred_section_label = section_encoder.inverse_transform(section_pred)\n",
    "print(f\"Predicted Section: {pred_section_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "* You must submit your assignment as a Jupyter notebook that will contain the full code and documentation of how you solved the questions, plus all accompanying material, such as embedding files, etc.\n",
    "\n",
    "* You are not required to upload your assignment; you may, if you wish, do your work in GitHub and submit a link to the private repository you will be using. If you do that, make sure to share the private repository with your instructor. \n",
    "\n",
    "* You may also include plain Python files that contain code that is called by your Jupyter notebook.\n",
    "\n",
    "* You must use [poetry](https://python-poetry.org/) for all dependency management. Somebody wishing to replicate your work should be able to do so by using the poetry file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code\n",
    "\n",
    "You understand that this is an individual assignment, and as such you must carry it out alone. You may discuss with your colleagues in order to better understand the questions, if they are not clear enough, but you should not ask them to share their answers with you, or to help you by giving specific advice. You can use ChatGPT or other chatbots, if you find them useful, along with traditional web search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
